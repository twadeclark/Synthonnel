[
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "Perplexity",
        "note": "",
        "providerUrl": "https://api.perplexity.ai",
        "parameters": "max_tokens          = 1024\nmax_time            = 60\ndo_sample           = True\n#top_k              = int\n#top_p              = int\n#temperature        = float\n#repetition_penalty = float",
        "apiKey": "",
        "model_options": ["sonar-small-chat", "sonar-small-online", "sonar-medium-chat", "sonar-medium-online", "mistral-7b-instruct", "mixtral-8x7b-instruct", "codellama-70b-instruct"]
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "Hugging Face Endpoint",
        "note": "Inference API",
        "providerUrl": "",
        "parameters": "max_tokens          = 1024\nmax_time            = 60\ndo_sample           = True\n#top_k              = int\n#top_p              = int\n#temperature        = float\n#repetition_penalty = float",
        "apiKey": ""
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "OpenAI",
        "note": "",
        "providerUrl": "https://api.openai.com/v1/",
        "parameters": "max_tokens         = 1024\ntimeout            = 60\n#frequency_penalty = float\n#logprobs          = bool\n#n                 = int\n#presence_penalty  = float\n#seed              = int\n#stop              = text\n#temperature       = float\n#top_logprobs      = int\n#top_p             = float\n#user              = text",
        "apiKey": "",
        "model_options": ["gpt-4-turbo", "gpt-4-turbo-2024-04-09", "gpt-4-turbo-preview", "gpt-4-0125-preview", "gpt-4-1106-preview", "gpt-4-vision-preview", "gpt-4-1106-vision-preview", "gpt-4", "gpt-4-0613", "gpt-4-32k", "gpt-4-32k-0613", "gpt-3.5-turbo-0125", "gpt-3.5-turbo", "gpt-3.5-turbo-1106", "gpt-3.5-turbo-instruct", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613"]
    },
    {
        "model": "(Local Model)",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "LM Studio",
        "note": "",
        "providerUrl": "http://localhost:1234/v1",
        "parameters": "#timeout           = float\n#frequency_penalty = float\n#logprobs          = bool\n#max_tokens        = int\n#n                 = int\n#presence_penalty  = float\n#seed              = int\n#stop              = text\n#temperature       = float\n#top_logprobs      = int\n#top_p             = float\n#user              = text",
        "apiKey": "none"
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "Hugging Face Free",
        "note": "Non-Streaming Serverless Inference API",
        "providerUrl": "https://api-inference.huggingface.co/models/",
        "parameters": "max_new_tokens      = 1024\nmax_time            = 60\ndo_sample           = True\n#top_k              = int\n#top_p              = int\n#temperature        = float\n#repetition_penalty = float",
        "apiKey": ""
    },
    {
        "model": "",
        "systemPrompt": "You are a gibberish bot. You respond with nonsense. Blah blah blah. That's all you do.",
        "provider": "Internal Testing",
        "note": "Internal Testing",
        "providerUrl": "Internal Testing",
        "parameters": "# Gibberish for Testing.",
        "apiKey": "none",
        "model_options": ["Lorem Ipsum Lightning", "Lorem Ipsum Fast", "Lorem Ipsum Standard"]
    }
]