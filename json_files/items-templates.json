[
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "IBM WatsonX",
        "note": "",
        "providerUrl": "https://us-south.ml.cloud.ibm.com/ml/v1/text/generation_stream?version=2023-05-29",
        "parameters": "########## REQUIRED\n# Either project_id or space_id must be specified.\n# project_id = \n# space_id = \n########## REQUIRED\n\nMIN_NEW_TOKENS          = 1\nMAX_NEW_TOKENS          = 1024\nTIME_LIMIT              = 60\n# DECODING_METHOD       = 'greedy' or 'sample'\n# LENGTH_PENALTY        = dictionary\n# TEMPERATURE           = float\n# TOP_P                 = float\n# TOP_K                 = integer\n# RANDOM_SEED           = integer\n# REPETITION_PENALTY    = float\n# STOP_SEQUENCES        = list\n# TRUNCATE_INPUT_TOKENS = integer\n",
        "apiKey": "",
        "model_options": ["codellama/codellama-34b-instruct-hf", "elyza/elyza-japanese-llama-2-7b-instruct", "google/flan-t5-xl", "google/flan-t5-xxl", "google/flan-ul2", "eleutherai/gpt-neox-20b", "ibm/granite-13b-chat-v2", "ibm/granite-13b-instruct-v1", "ibm/granite-13b-instruct-v2", "ibm/granite-20b-multilingual", "meta-llama/llama-2-13b-chat", "meta-llama/llama-2-70b-chat", "ibm-mistralai/mixtral-8x7b-instruct-v01-q", "ibm/mpt-7b-instruct2", "bigscience/mt0-xxl", "bigcode/starcoder"]
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "GoogleAI",
        "note": "",
        "providerUrl": "(not used)",
        "parameters": "max_output_tokens = 1024\n#temperature      = float\n#top_p            = float\n#top_k            = int\n",
        "apiKey": "",
        "model_options": ["models/gemini-1.0-pro", "models/gemini-1.0-pro-001", "models/gemini-1.0-pro-latest", "models/gemini-1.5-pro-latest"]
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "Groq",
        "note": "",
        "providerUrl": "https://api.groq.com/openai/v1",
        "parameters": "max_tokens          = 1024\nmax_time            = 60\ndo_sample           = True\n#top_k              = int\n#top_p              = int\n#temperature        = float\n#repetition_penalty = float",
        "apiKey": "",
        "model_options": ["gemma-7b-it", "llama2-70b-4096", "llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768"]
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "Perplexity",
        "note": "",
        "providerUrl": "https://api.perplexity.ai",
        "parameters": "max_tokens          = 1024\nmax_time            = 60\ndo_sample           = True\n#top_k              = int\n#top_p              = int\n#temperature        = float\n#repetition_penalty = float",
        "apiKey": "",
        "model_options": ["sonar-small-chat", "sonar-small-online", "sonar-medium-chat", "sonar-medium-online", "mistral-7b-instruct", "mixtral-8x7b-instruct", "codellama-70b-instruct"]
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "Hugging Face Endpoint",
        "note": "Inference API",
        "providerUrl": "",
        "parameters": "max_tokens          = 1024\nmax_time            = 60\ndo_sample           = True\n#top_k              = int\n#top_p              = int\n#temperature        = float\n#repetition_penalty = float",
        "apiKey": ""
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "OpenAI",
        "note": "",
        "providerUrl": "https://api.openai.com/v1/",
        "parameters": "max_tokens         = 1024\ntimeout            = 60\n#frequency_penalty = float\n#logprobs          = bool\n#n                 = int\n#presence_penalty  = float\n#seed              = int\n#stop              = text\n#temperature       = float\n#top_logprobs      = int\n#top_p             = float\n#user              = text",
        "apiKey": "",
        "model_options": ["gpt-4o-2024-05-13", "gpt-4o", "gpt-4-turbo", "gpt-4-turbo-2024-04-09", "gpt-4-turbo-preview", "gpt-4-0125-preview", "gpt-4-1106-preview", "gpt-4-vision-preview", "gpt-4-1106-vision-preview", "gpt-4", "gpt-4-0613", "gpt-4-32k", "gpt-4-32k-0613", "gpt-3.5-turbo-0125", "gpt-3.5-turbo", "gpt-3.5-turbo-1106", "gpt-3.5-turbo-instruct", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613"]
    },
    {
        "model": "Local Model",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "LM Studio",
        "note": "",
        "providerUrl": "http://localhost:1234/v1",
        "parameters": "#timeout           = float\n#frequency_penalty = float\n#logprobs          = bool\n#max_tokens        = int\n#n                 = int\n#presence_penalty  = float\n#seed              = int\n#stop              = text\n#temperature       = float\n#top_logprobs      = int\n#top_p             = float\n#user              = text",
        "apiKey": "none"
    },
    {
        "model": "",
        "systemPrompt": "You are a helpful assistant who provides brief answers to user questions.",
        "provider": "Hugging Face Free",
        "note": "Non-Streaming Serverless Inference API",
        "providerUrl": "https://api-inference.huggingface.co/models/",
        "parameters": "max_new_tokens      = 1024\nmax_time            = 60\ndo_sample           = True\n#top_k              = int\n#top_p              = int\n#temperature        = float\n#repetition_penalty = float",
        "apiKey": ""
    },
    {
        "model": "",
        "systemPrompt": "",
        "provider": "Generic OpenAI Interface",
        "note": "",
        "providerUrl": "",
        "parameters": "",
        "apiKey": ""
    },
    {
        "model": "",
        "systemPrompt": "You are a gibberish bot. You respond with nonsense. Blah blah blah. That's all you do.",
        "provider": "Internal Testing",
        "note": "Internal Testing",
        "providerUrl": "Internal Testing",
        "parameters": "# Gibberish for Testing.",
        "apiKey": "none",
        "model_options": ["Lorem Ipsum Lightning", "Lorem Ipsum Fast", "Lorem Ipsum Standard", "Lorem Ipsum Slow", "Lorem Ipsum Crawl"]
    }
]